{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ae01d0",
   "metadata": {},
   "source": [
    "# Exp4. 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd4982",
   "metadata": {},
   "source": [
    "# 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887a7ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기 :  187088\n",
      "Examples : \n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)    # glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트로 반환\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기 : \", len(raw_corpus))\n",
    "print(\"Examples : \\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787121dc",
   "metadata": {},
   "source": [
    "# 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20981db",
   "metadata": {},
   "source": [
    "### 2-1. 길이가 0인 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ea385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "    if idx > 9: break     # 10개의 문장만 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f197e",
   "metadata": {},
   "source": [
    "### 2_2. 필터링 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac7d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()  # 소문자로 변경, 양쪽 공백 지우기\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)  # 특수문자 양쪽에 공백 넣기\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # 여러개의 공백은 하나의 공백으로 변경\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # 괄호 안 특수문자가 아닌 모든 문자를 하나의 공백으로 변경\n",
    "    sentence = sentence.strip()   # 양쪽 공백 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419edd3a",
   "metadata": {},
   "source": [
    "### 2-3. corpus 생성 (정규표현식 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b44de6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split()) > 15: continue  # 토큰의 개수가 15개 넘어가는 문장 제외\n",
    "        \n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae92ec",
   "metadata": {},
   "source": [
    "### 2-4. corpus를 tensor로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528c745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2971 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  117 ...    0    0    0]\n",
      " [   2  258  195 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc0788c2ac0>\n"
     ]
    }
   ],
   "source": [
    "# tokenizer는 문장으로부터 단어를 토큰화하고 숫자에 대응하는 딕셔너리를 사용할 수 있도록 함\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words = 12000, # 단어장의 크기는 12,000 이상으로 설정\n",
    "    filters = ' ',     # 이미 문장을 정제했으니 filters 필요 없음\n",
    "    oov_token = \"<unk>\"# 단어장에 포함되지 않은 단어를 '<unk>'로 변경\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)    # 문자 데이터를 입력받아 리스트 형태로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # 단어를 숫자의 시퀀스 형태로 변환\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')   # 같은 길이의 시퀀스로 변환 (padding)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513ca6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6269    3    0    0    0\n",
      "     0]\n",
      " [   2   15 2971  872    5    8   11 5747    6  374    3    0    0    0\n",
      "     0]\n",
      " [   2   33    7   40   16  164  288   28  333    5   48    7   46    3\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer에 구축된 단어사전의 인덱스 확인\n",
    "\n",
    "print(tensor[:3, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "094a2691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# tokenizer에 구축된 단어사전 내용 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10:break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293e31f",
   "metadata": {},
   "source": [
    "# 3. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c04a2a",
   "metadata": {},
   "source": [
    "### 3-1.소스 문장 & 타겟 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02178ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6269    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6269    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # 마지막 토큰 자름 (총 14개) → 소스 문장\n",
    "tgt_input = tensor[:, 1:]  # 첫번째 토큰(<start>) 자름 (총 14개) → 타겟 문장\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd3ace",
   "metadata": {},
   "source": [
    "### 3-2. 훈련데이터와 평가데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90aa9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n",
      "Source Val: (31246, 14)\n",
      "Target Val: (31246, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 42)\n",
    "# 총 데이터의 20%를 평가 데이터셋으로 사용\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Val:\", enc_val.shape)\n",
    "print(\"Target Val:\", dec_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce22c3",
   "metadata": {},
   "source": [
    "### 3-3. tf.data.Dataset 객체로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45795796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1 # 단어사전 12,000개 + <pad> = 12,001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)) # 데이터 셋 객체로 변환\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce9371",
   "metadata": {},
   "source": [
    "# 4. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af169f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 518   # 워드 벡터의 차원수 (단어가 추상적으로 표현되는 크기)\n",
    "hidden_size = 2048     # 모델에 얼마나 많은 일꾼을 둘 것인가?\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "# Embedding 레이어는 인덱스 값을 해당 인덱스번째의 워드 벡터로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7c6553a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 8.14897940e-05,  5.82887369e-05,  5.13098435e-04, ...,\n",
       "          1.79299459e-04, -1.21782236e-04, -4.18903161e-04],\n",
       "        [ 3.60988546e-04,  7.36650662e-04,  6.58627658e-04, ...,\n",
       "          3.78477038e-04, -7.96392778e-05, -3.90958710e-04],\n",
       "        [ 5.84731228e-04,  9.18104663e-04,  6.11621246e-04, ...,\n",
       "          4.73846565e-04,  1.70839354e-04, -3.41477833e-04],\n",
       "        ...,\n",
       "        [ 1.45805941e-03,  2.40732334e-05, -6.18712947e-05, ...,\n",
       "          1.60406227e-03,  1.64401915e-03,  6.37070392e-04],\n",
       "        [ 1.38485013e-03, -1.92399253e-04, -3.04381101e-04, ...,\n",
       "          1.91372214e-03,  2.01442582e-03,  5.95048303e-04],\n",
       "        [ 1.34715601e-03, -4.28532541e-04, -4.95773915e-04, ...,\n",
       "          2.29756301e-03,  2.32371432e-03,  4.79921087e-04]],\n",
       "\n",
       "       [[ 8.14897940e-05,  5.82887369e-05,  5.13098435e-04, ...,\n",
       "          1.79299459e-04, -1.21782236e-04, -4.18903161e-04],\n",
       "        [ 2.22071933e-04, -1.12079186e-04,  8.90086114e-04, ...,\n",
       "          1.11128167e-04, -5.10910468e-04, -5.10548925e-05],\n",
       "        [ 4.03429673e-04,  2.71351979e-04,  1.05952076e-03, ...,\n",
       "          7.95490778e-05, -6.92364818e-04,  2.56350148e-04],\n",
       "        ...,\n",
       "        [ 5.96339290e-04, -1.04919588e-03,  5.24383096e-04, ...,\n",
       "          3.64630530e-03,  1.85759587e-03,  9.70346096e-04],\n",
       "        [ 6.83312537e-04, -1.26427808e-03,  3.69443791e-04, ...,\n",
       "          4.09017270e-03,  2.05367710e-03,  7.46221864e-04],\n",
       "        [ 7.86629156e-04, -1.45939970e-03,  2.35911706e-04, ...,\n",
       "          4.51998599e-03,  2.18633609e-03,  5.13502280e-04]],\n",
       "\n",
       "       [[ 8.14897940e-05,  5.82887369e-05,  5.13098435e-04, ...,\n",
       "          1.79299459e-04, -1.21782236e-04, -4.18903161e-04],\n",
       "        [-1.61110202e-05,  1.85456593e-05,  9.93348309e-04, ...,\n",
       "          4.38226096e-04, -2.28670164e-04, -6.02881133e-04],\n",
       "        [-5.17648412e-04, -2.40232301e-04,  1.05341873e-03, ...,\n",
       "          6.18180144e-04, -1.80575822e-04, -4.98153735e-04],\n",
       "        ...,\n",
       "        [-8.67445022e-04,  3.43480606e-05, -1.19519493e-04, ...,\n",
       "          1.48145668e-03,  4.35277674e-04, -7.43158802e-04],\n",
       "        [-7.82557763e-04, -1.04888804e-05, -3.06036789e-04, ...,\n",
       "          1.87936728e-03,  9.45103355e-04, -8.40407796e-04],\n",
       "        [-6.29149727e-04, -1.54110894e-04, -4.75818670e-04, ...,\n",
       "          2.33577774e-03,  1.44395791e-03, -9.31370480e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.14897940e-05,  5.82887369e-05,  5.13098435e-04, ...,\n",
       "          1.79299459e-04, -1.21782236e-04, -4.18903161e-04],\n",
       "        [-1.61110202e-05,  1.85456593e-05,  9.93348309e-04, ...,\n",
       "          4.38226096e-04, -2.28670164e-04, -6.02881133e-04],\n",
       "        [-2.67708965e-04,  3.70470749e-04,  8.90396652e-04, ...,\n",
       "          6.63329090e-04, -5.67768147e-05, -6.34026655e-04],\n",
       "        ...,\n",
       "        [-6.41124207e-04, -1.11798372e-03, -1.44168339e-03, ...,\n",
       "         -7.61098636e-04, -2.44740746e-04,  2.59628519e-04],\n",
       "        [-5.87748713e-04, -1.42716605e-03, -1.29364210e-03, ...,\n",
       "         -8.46356794e-04, -3.78931902e-04,  2.04056967e-04],\n",
       "        [-7.37835362e-04, -1.64834084e-03, -1.08546333e-03, ...,\n",
       "         -1.00572105e-03, -3.82922211e-04,  3.94234143e-04]],\n",
       "\n",
       "       [[ 8.14897940e-05,  5.82887369e-05,  5.13098435e-04, ...,\n",
       "          1.79299459e-04, -1.21782236e-04, -4.18903161e-04],\n",
       "        [ 1.74716275e-04,  3.43320833e-04,  1.19412143e-03, ...,\n",
       "          2.11326129e-04, -4.42077086e-04, -7.17776536e-04],\n",
       "        [-1.76330889e-06,  4.02011094e-04,  1.52514735e-03, ...,\n",
       "          2.20154296e-04, -8.01744056e-04, -2.80701701e-04],\n",
       "        ...,\n",
       "        [ 3.29843257e-04, -6.85049512e-04,  3.65536165e-04, ...,\n",
       "          3.68038937e-03,  2.23610876e-03,  1.20136399e-04],\n",
       "        [ 5.43282076e-04, -9.31955234e-04,  2.35584172e-04, ...,\n",
       "          4.21020156e-03,  2.37372378e-03, -7.35455615e-05],\n",
       "        [ 7.45440368e-04, -1.16351712e-03,  1.28610525e-04, ...,\n",
       "          4.70581604e-03,  2.45720102e-03, -2.59484659e-04]],\n",
       "\n",
       "       [[ 8.14897940e-05,  5.82887369e-05,  5.13098435e-04, ...,\n",
       "          1.79299459e-04, -1.21782236e-04, -4.18903161e-04],\n",
       "        [-1.68735918e-04,  6.53178577e-05,  9.74208640e-04, ...,\n",
       "          4.47109807e-04, -1.10898654e-05, -7.61652598e-04],\n",
       "        [-6.23972155e-05, -8.76257327e-05,  1.46647170e-03, ...,\n",
       "          9.14543285e-04,  3.71701113e-04, -1.00875704e-03],\n",
       "        ...,\n",
       "        [ 1.75934547e-04, -7.41454889e-04,  5.76493505e-04, ...,\n",
       "          3.29154334e-03,  2.12347461e-03, -1.58891140e-04],\n",
       "        [ 3.55455064e-04, -9.57586744e-04,  4.02737234e-04, ...,\n",
       "          3.73553787e-03,  2.32305424e-03, -2.96731218e-04],\n",
       "        [ 5.29188896e-04, -1.16378162e-03,  2.61941750e-04, ...,\n",
       "          4.17871075e-03,  2.45757401e-03, -4.35048860e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "    \n",
    "model(src_sample)\n",
    "\n",
    "# 256 : 이전 스텝에서 지정한 배치 사이즈 (256개의 문장 데이터)\n",
    "# 14 : LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력\n",
    "# 12001 : Dence 레이어의 출력 차원수 (12001개의 단어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19c9dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6216518   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  21028864  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 85,398,055\n",
      "Trainable params: 85,398,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5e53c",
   "metadata": {},
   "source": [
    "# 5. 모델 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9bd96",
   "metadata": {},
   "source": [
    "* [조건] 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4e659",
   "metadata": {},
   "source": [
    "[1차 시도]\n",
    "* embedding_size = 256\n",
    "* hidden_size = 1024\n",
    "* bach_size = 256\n",
    "* epochs = 10\n",
    "\n",
    "loss : 2.20 / val_loss : 2.51\n",
    "\n",
    "[2차 시도]\n",
    "* embedding_size = 256\n",
    "* hidden_size = 1024\n",
    "* bach_size = 128\n",
    "* epochs = 10\n",
    "\n",
    "loss : 1.27 / val_loss : 2.44  \n",
    "→ i love you , i m a liability\n",
    "\n",
    "[3차 시도]\n",
    "* embedding_size = 256\n",
    "* hidden_size = 2048\n",
    "* bach_size = 128\n",
    "* epochs = 10\n",
    "\n",
    "loss : 1.11 / val_loss : 2.24  \n",
    "→ i love the way you lie  \n",
    "▶ Epochs 9부터 val_loss가 다시 올리감\n",
    "\n",
    "[4차 시도]\n",
    "* embedding_size = 512\n",
    "* hidden_size = 2048\n",
    "* bach_size = 128\n",
    "* epochs = 10\n",
    "\n",
    "loss : 0.99 / val_loss : 2.24  \n",
    "▶ Epoch 7부터 val_loss가 다시 올라감\n",
    "\n",
    "[5차 시도]\n",
    "* embedding_size = 512\n",
    "* hidden_size = 2048\n",
    "* bach_size = 128\n",
    "* epochs = 8\n",
    "\n",
    "loss : 1.05 / val_loss : 2.19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c92dc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "977/977 [==============================] - 284s 288ms/step - loss: 3.1092 - val_loss: 2.7864\n",
      "Epoch 2/8\n",
      "977/977 [==============================] - 291s 298ms/step - loss: 2.5643 - val_loss: 2.5040\n",
      "Epoch 3/8\n",
      "977/977 [==============================] - 292s 299ms/step - loss: 2.1661 - val_loss: 2.3231\n",
      "Epoch 4/8\n",
      "977/977 [==============================] - 292s 299ms/step - loss: 1.7989 - val_loss: 2.2111\n",
      "Epoch 5/8\n",
      "977/977 [==============================] - 292s 299ms/step - loss: 1.4962 - val_loss: 2.1545\n",
      "Epoch 6/8\n",
      "977/977 [==============================] - 292s 299ms/step - loss: 1.2742 - val_loss: 2.1423\n",
      "Epoch 7/8\n",
      "977/977 [==============================] - 292s 299ms/step - loss: 1.1301 - val_loss: 2.1689\n",
      "Epoch 8/8\n",
      "977/977 [==============================] - 292s 299ms/step - loss: 1.0526 - val_loss: 2.1944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc060414af0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer)\n",
    "model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val), epochs = 8, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdc804",
   "metadata": {},
   "source": [
    "# 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea6166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])  # 테스트를 위해 init_sentence도 텐서로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor)    # 입력받은 문장의 텐서 입력\n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1910239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so , hey <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2607247",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a537749",
   "metadata": {},
   "source": [
    "### 1) 학습시간"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2208d6",
   "metadata": {},
   "source": [
    "이번 프로젝트를 하면서 다양한 시도를 하지 못한 것이 조금 아쉬웠다.  \n",
    "학습시간이 너무나 오래 걸려서 이전 프로젝트와 같이 여러 시도를 해보지 못한 것이 아쉽다.  \n",
    "텍스트 생성 모델을 학습시킬 때 원래 이렇게 학습시간이 오래 걸리는 것인지, 이것을 해결할 수 있는 방법이 있는지 궁금해졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91eca26",
   "metadata": {},
   "source": [
    "### 2) val_loss 맞추기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d120d86",
   "metadata": {},
   "source": [
    "val_loss를 2.2 이하로 나오도록 맞추기 위해 다양한 시도를 하였다. 하이퍼파라미터값을 조정을 하였는데 확실히 val_loss 값이 내려가는 것을 확인할 수 있었다.  \n",
    "하지만 epoch = 7 이후로는 내려갔던 val_loss 값이 다시 올라갔다. 그래서 embedding_size를 변경해서 다시 돌려보았으나 이 때도 똑같이 다시 올라간 것을 확인할 수 있었다.  \n",
    "그래서 epoch를 줄이면 되지 않을까라고 생각을 해서 epoch를 8로 변경한 후 진행하였고, '2.19'라는 값을 얻게 되었다.  \n",
    "사실 epoch 값을 줄이지 않고 다른 하이퍼 파라미터를 조정하면 되었을 수 있었는데, 제출 기한이 있다보니 여러 시도를 해보지 못하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18762901",
   "metadata": {},
   "source": [
    "### 3) 생성된 가사"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac453bb",
   "metadata": {},
   "source": [
    "사실 val_loss를 맞추는 것에 너무나 초점을 두어 돌릴 때마다 생성된 문장을 몇 개만 확인하였다.  \n",
    "val_loss가 2.2 이하로 내려갔을 때 나온 문장은 'i love you so , hey'이다. 조금 더 다양한 단어를 사용한 문장이 나왔으면 하는 바람이 있어서 그런지 생성된 문장은 다소 아쉬웠다.  \n",
    "val_loss가 2.4 정도 되었을 때 생성된 문장은 아래와 같다.\n",
    "*  i love you , i m a liability\n",
    "* i love the way you lie  \n",
    "\n",
    "첫번째 문장을 보면 val_loss가 높아도 괜찮은 문장이 생성되었다. 매우 사랑꾼 같은 문장이 생성되었다.  \n",
    "하지만 두번째 문장은 조금 부정적인 느낌이 들었다. hidden_size를 2배로 높여서 이런 문제가 발생하는 건가라는 생각이 들었다.  \n",
    "\n",
    "▶ val_loss를 맞추는 것도 중요하지만 어떠한 문장이 생성되었는지, 문맥에 잘 맞는 문장인지, 또 예상과 다르게 부정적인 문장인지 확인을 해서 하이퍼파라미터 등 다양한 방법으로 모델을 구축하는 것도 중요하다고 생각했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea16c59",
   "metadata": {},
   "source": [
    "### 4) 추가로 시도해 볼 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ac734",
   "metadata": {},
   "source": [
    "* epoch 값을 10으로 유지한채로 다른 하이퍼파라미터를 조정하였을 때 val_loss 값을 2.2로 맞추는 방법\n",
    "* hidden_size에 따라 문장이 어떻게 생성되는지 확인하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
